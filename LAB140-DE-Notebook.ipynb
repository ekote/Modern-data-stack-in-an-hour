{"cells":[{"cell_type":"markdown","source":["# 1. Access the lakehouse\n","\n","![image-alt-text](https://github.com/ekote/azure-architect/blob/master/images/lakehouse_overview.gif?raw=true)\n"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["# 2. Lakehouse artifact overview\n","\n","![image-alt-text](https://github.com/ekote/azure-architect/blob/master/images/2_30frames.gif?raw=true)"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["# 3. Get data from the lakehouse\n","\n","![image-alt-text](https://github.com/ekote/azure-architect/blob/master/images/3.gif?raw=true)"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["`df = spark.sql(\"SELECT * FROM lab140lakehouse.nyc_taxi LIMIT 1000\")` - This line of code uses the `spark.sql()` function to run an SQL query on a table called `nyc_taxi` located in the lakehouse `lab140lakehouse`. The query selects all columns `(*)` from the table and limits the result to the first 1000 rows with the `LIMIT 1000` clause. The result of the query is then stored in a PySpark DataFrame called `df`.\n","\n","`display(df)` - the `display()` function is used to visualize the contents of a DataFrame in a tabular format. In this case, it visualizes the contents of the df DataFrame created in the previous line."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["df = spark.sql(\"SELECT * FROM lab140lakehouse.nyc_taxi LIMIT 1000\")\n","\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T05:32:15.4503827Z\",\"session_start_time\":\"2023-05-04T05:32:15.6213464Z\",\"execution_start_time\":\"2023-05-04T05:32:24.2650713Z\",\"execution_finish_time\":\"2023-05-04T05:32:45.5055462Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false}},{"cell_type":"markdown","source":["The output of `df.printSchema()` displays the name of each column in the DataFrame, the data type of each column, and whether null values are allowed. This information can be useful for understanding the structure of the data in the DataFrame, and for performing operations on the data."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["df.printSchema()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T05:43:43.6196943Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T05:43:43.9246186Z\",\"execution_finish_time\":\"2023-05-04T05:43:44.3008892Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"markdown","source":["The code `df.show(5)` is used to display the first five rows of a DataFrame called df. This is a useful function when working with large datasets to quickly inspect the data and ensure that it has been loaded correctly. The number inside the parenthesis specifies the number of rows to display. In this case, the function call displays the first five rows of the DataFrame `df`. Each row is displayed as a separate line and the columns are separated by vertical bars."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["df.show(5)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T05:43:46.2435173Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T05:43:46.5695667Z\",\"execution_finish_time\":\"2023-05-04T05:43:48.3372628Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"markdown","source":["When working with data, one of the initial tasks is to read it into the environment for analysis. Once the data is loaded, basic analysis such as filtering, sorting, and aggregating can be performed. However, as the scale and complexity of the data increase, there is a need for more advanced data engineering scenarios such as data cleansing, transformation, and aggregation. \n","\n","Congratulations on completing the first part of the sample. Let us now move on to custom scenarios."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["# 4. Data cleaning and transformation\n","\n","In this scenario, the data engineer could perform some data cleaning and transformation tasks to prepare the data for downstream analysis. \n","\n","Objective: **Cleanse the data and filter out invalid records for further analysis.**\n","\n","In this scenario, we aim to demonstrate how data engineers can perform data cleansing and filtering on a large dataset. We begin by loading the data from the source and then filter out records where the trip distance and fare amount are less than or equal to zero, which are invalid records.\n","\n","Next, we cleanse the data by converting the `store_and_fwd_flag` column to a boolean type, and converting the `lpep_pickup_datetime` and `lpep_dropoff_datetime` columns to timestamp types. Finally, we write the cleansed data to the destination in the parquet format.\n","\n","This scenario demonstrates the importance of data cleansing and filtering to ensure the data is accurate and valid before proceeding with further analysis."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, when\n","\n","# Load data from source\n","df = spark.read.load(\"Tables/nyc_taxi\", header=True, inferSchema=True)\n","df_count = df.count()\n","\n","# Remove invalid records\n","df = df.filter((col(\"tripDistance\") > 0) & (col(\"fareAmount\") > 0))\n","df_count_after_clearning = df.count()\n","\n","number_of_deleted_records = df_count - df_count_after_clearning\n","\n","print(f\"Removed {number_of_deleted_records} records\")\n","\n","# # Cleanse data\n","df = df.withColumn(\"storeAndFwdFlag\", when(col(\"storeAndFwdFlag\") == \"Y\", True).otherwise(False))\n","df = df.withColumn(\"lpepPickupDatetime\", col(\"lpepPickupDatetime\").cast(\"timestamp\"))\n","df = df.withColumn(\"lpepDropoffDatetime\", col(\"lpepDropoffDatetime\").cast(\"timestamp\"))\n","\n","# Display cleansed data to destination\n","display(df)\n","\n","# Write cleansed data to destination\n","df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"nyc_taxi_cleansed\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T05:45:31.5663427Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T05:45:31.8887633Z\",\"execution_finish_time\":\"2023-05-04T05:45:45.2715868Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false}},{"cell_type":"markdown","source":["### Refresh lakehouse to see the results\n","\n","![image-alt-text](https://github.com/ekote/azure-architect/blob/master/images/4.gif?raw=true)"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["# 5. Exploratory data analysis (EDA)\n","\n","Exploratory data analysis (EDA) is a common scenario for data engineers. EDA is the process of analyzing and understanding data to gain insights, identify patterns, and develop hypotheses for further investigation. In data engineering, EDA is often done to identify data quality issues, anomalies, or other problems that need to be addressed before data can be used for analysis or modeling. EDA can also help data engineers to understand the relationships between different data sources and determine the best way to join or transform them.\n","\n","`df.count()` is a Spark DataFrame API function that returns the number of rows in the DataFrame. It is a convenient way to quickly determine the size of the DataFrame without having to iterate over all the rows manually. The function is an action in Spark, meaning it triggers a computation that counts the number of rows in the DataFrame and returns the result. It is useful for getting a quick overview of the data size and checking if any rows are missing or dropped during data processing. However, it should be used with caution on large datasets, as it can be a costly operation that requires significant computational resources."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Load data from source\n","df = spark.read.load(\"Tables/nyc_taxi_cleansed\", header=True, inferSchema=True)\n","\n","# Count the number of rows \n","df.count()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T05:51:35.9877444Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T05:51:36.3483798Z\",\"execution_finish_time\":\"2023-05-04T05:51:37.3933509Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"markdown","source":["`df.dtypes` is an attribute of a DataFrame object that returns a list of tuples containing the column names and their corresponding data types. The data types are represented using the Spark SQL DataType class, which is a set of classes for representing data types in Spark SQL."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Display the data types of the columns.\n","\n","df.dtypes"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T05:51:40.569974Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T05:51:40.8850041Z\",\"execution_finish_time\":\"2023-05-04T05:51:41.228206Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"markdown","source":["The code imports the col function from `pyspark.sql.functions` and uses it to select the `\"vendorID\"` column from the Spark DataFrame `df`. The `groupBy()` function is then called on the resulting column object to group the DataFrame by the distinct values in the `\"vendorID\"` column. The `count()` function is then applied to the resulting grouped DataFrame to calculate the number of records in each group. Finally, the `show()` function is used to display the resulting DataFrame on the console."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Group the data by 'VendorID' and count the number of rows in each group. \n","\n","from pyspark.sql.functions import col\n","\n","df.groupBy(col(\"vendorID\")).count().show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T05:52:16.893333Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T05:52:17.3303332Z\",\"execution_finish_time\":\"2023-05-04T05:52:19.0038804Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"markdown","source":["The code reads the Spark DataFrame `df` which contains information about NYC taxi trips. The code uses the `'min'` and `'max'` functions from PySpark to select the earliest and latest pickup dates respectively. These dates are stored in the variables `'oldest_day'` and `'latest_day'`. The `'collect'` function is then used to retrieve these values and they are printed to the console using the `'print'` function. The output displays the earliest and latest pickup dates in the dataset."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Retrieve information about the earliest and latest pickup dates in the dataset.\n","\n","from pyspark.sql.functions import min, max\n","\n","oldest_day = df.select(min(\"lpepPickupDatetime\")).collect()[0][0]\n","latest_day = df.select(max(\"lpepDropoffDatetime\")).collect()[0][0]\n","\n","print(\"Oldest pickup date: \", oldest_day)\n","print(\"Latest pickup date: \", latest_day)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T05:53:06.2817683Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T05:53:06.7137755Z\",\"execution_finish_time\":\"2023-05-04T05:53:07.6623463Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"markdown","source":["This code uses the PySpark `date_format` function to group the `df` DataFrame by the year, month, and day of the `lpep_pickup_datetime` column, and then counts the number of occurrences for each date.\n","\n","`date_format` is a PySpark SQL function used to format the date or timestamp column to the specified format. In this code, the format used is `yyyy-MM-dd`. The alias `pickup_date` is assigned to the formatted date column, and the DataFrame is grouped by this column using the `groupby()` method. The `count()` method is then applied to count the number of occurrences of each pickup_date. Finally, the result is displayed using the `show()` method."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["from pyspark.sql.functions import date_format\n","\n","# group by year, month and day of lpepPickupDatetime\n","df_grouped = df.groupby(date_format('lpepPickupDatetime', 'yyyy-MM-dd').alias('pickup_date')).count()\n","\n","# show the result\n","df_grouped.show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T05:53:59.2072471Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T05:53:59.4794978Z\",\"execution_finish_time\":\"2023-05-04T05:54:01.1825459Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"markdown","source":["This code computes the minimum and maximum values of the fare_amount column in the Spark DataFrame df. It uses the `min()` and `max()` functions from the `pyspark.sql.functions` module to compute the minimum and maximum values, respectively. The `alias()` method is used to rename the resulting columns as `\"min\"` and `\"max\"`. Finally, the `show()` method is used to display the resulting DataFrame with two columns `\"min\"` and `\"max\"`, showing the minimum and maximum values of the `fare_amount column`."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# min max values of target feature \"fare_amount\"\n","\n","df.select(min('fareAmount').alias('min'), max('fareAmount').alias('max')).show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T05:54:50.0109327Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T05:54:50.316545Z\",\"execution_finish_time\":\"2023-05-04T05:54:51.3894068Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"markdown","source":["This code is performing descriptive statistical analysis on the `\"fare_amount\"` column of a Spark DataFrame named `\"df\"`. Specifically, it is using the `describe()` method of the DataFrame to compute summary statistics including `count`, `mean`, `standard deviation`, `minimum`, and `maximum`.\n","\n","The result of `describe()` is then converted to a Pandas DataFrame using the `toPandas()` method. This allows the statistics to be displayed in a more user-friendly table format, which includes the same summary statistics along with the 25th, 50th, and 75th percentiles. The resulting table provides insights into the central tendency and dispersion of the `\"fare_amount\"` variable, and can be useful for understanding the distribution of the data and identifying potential outliers."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# General statistical characteristics of fare amount\n","\n","df.select('fareAmount').describe().toPandas()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T05:55:29.575943Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T05:55:29.8544171Z\",\"execution_finish_time\":\"2023-05-04T05:55:30.8384268Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"markdown","source":["This code computes the approximate quantiles of the `'fare_amount'` column of the DataFrame `'df'` using the `'approxQuantile'` function from PySpark's SQL functions module. The function takes three arguments - the name of the column for which quantiles are to be computed, the list of quantile values to be returned, and a relative error value. In this case, the quantiles are 0.1, 0.25, 0.5, 0.75, and 0.9, and the relative error is set to 0.01. The function returns an array of approximate quantile values for the given column and quantile values."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# quantiles\n","\n","df.select('fareAmount').approxQuantile(\"fareAmount\",[0.1, 0.25, 0.5, 0.75, 0.9], 0.01)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T05:56:03.7707791Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T05:56:04.0599033Z\",\"execution_finish_time\":\"2023-05-04T05:56:05.7889833Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"markdown","source":["This code is used to plot the distribution of fare_amount using matplotlib library in Python. The fare_amount data is first extracted from the Spark DataFrame using the select function along with the F.col() function to extract the fare_amount column. The resulting DataFrame is then converted to a Pandas DataFrame using the toPandas() function. The fare_amount data is then plotted as a histogram using the hist() function from matplotlib. The number of bins for the histogram is set to 50 using the bins parameter. Finally, the title and axis labels for the plot are set using the title(), xlabel(), and ylabel() functions, and the plot is displayed using the show() function."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import pyspark.sql.functions as F\n","\n","# Assuming your DataFrame is named `df`\n","fare_distribution = df.select(F.col('fareAmount')).toPandas()\n","\n","# Plot histogram\n","plt.hist(fare_distribution, bins=50)\n","plt.title('Distribution of fareAmount')\n","plt.xlabel('fareAmount')\n","plt.ylabel('Frequency')\n","plt.show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T05:56:09.0802203Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T05:56:09.3690793Z\",\"execution_finish_time\":\"2023-05-04T05:56:10.3201262Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"markdown","source":["This code snippet demonstrates how to create a scatter plot using Matplotlib in Python. The code assumes that the Spark DataFrame df contains the columns fare_amount and trip_distance. First, the Spark DataFrame is converted to a Pandas DataFrame using the toPandas() function. Then, a scatter plot is created using ax.scatter() function. The x and y arguments of the scatter() function represent the variables to be plotted on the x- and y-axes, respectively. The alpha argument controls the transparency of the points in the scatter plot. The axis labels and title are set using the ax.set_xlabel(), ax.set_ylabel(), and ax.set_title() functions. Finally, the plot is displayed using the plt.show() function. This code can be used to visualize the correlation between fare amount and trip distance in the DataFrame."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# assuming `df` is your Spark DataFrame containing the columns `fare_amount` and `trip_distance`\n","\n","# convert Spark DataFrame to Pandas DataFrame\n","df_pd = df.select(['fareAmount', 'tripDistance']).toPandas()\n","\n","# create scatter plot\n","fig, ax = plt.subplots()\n","ax.scatter(x=df_pd['tripDistance'], y=df_pd['fareAmount'], alpha=0.5)\n","\n","# set axis labels and title\n","ax.set_xlabel('Trip Distance')\n","ax.set_ylabel('Fare Amount')\n","ax.set_title('Correlation between Fare Amount and Trip Distance')\n","\n","# show the plot\n","plt.show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T05:56:26.066225Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T05:56:26.3712409Z\",\"execution_finish_time\":\"2023-05-04T05:56:28.9968001Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"markdown","source":["# 6. Data aggregation and summarization\n","\n","In this scenario, the data engineer could aggregate and summarize the data to provide insights into the overall trends and patterns in the dataset. For example, they could group the data by some categorical columns (such as VendorID or RatecodeID) and calculate some summary statistics for the numerical columns (such as average fare_amount or total trip_distance). This could involve using Spark's built-in aggregation functions (such as groupBy and agg) to perform these calculations."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["The code calculates the average fare amount per month by grouping the DataFrame df by year and month of the lpep_pickup_datetime column. It uses the avg function from the pyspark.sql.functions module to calculate the average fare amount and aliases the resulting column as \"average_fare\". The resulting DataFrame average_fare_per_month is sorted by year and month and is displayed using the display function. Finally, the code saves the results to a new delta table named \"average_fare_per_month\" using the write function with \"delta\" format, and \"overwrite\" mode."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, year, month, dayofmonth, avg\n","\n","# Calculate average fare amount per month\n","average_fare_per_month = (\n","    df\n","    .groupBy(year(\"lpepPickupDatetime\").alias(\"year\"), month(\"lpepPickupDatetime\").alias(\"month\"))\n","    .agg(avg(\"fareAmount\").alias(\"average_fare\"))\n","    .orderBy(\"year\", \"month\")\n",")\n","display(average_fare_per_month)\n","\n","# Save the results to a new delta table\n","average_fare_per_month.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"average_fare_per_month\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T05:56:50.0604964Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T05:56:50.4038143Z\",\"execution_finish_time\":\"2023-05-04T05:57:03.2838515Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false}},{"cell_type":"markdown","source":["The code calculates the average fare amount per day by grouping the DataFrame df by year and month and day of the lpep_pickup_datetime column. It uses the avg function from the pyspark.sql.functions module to calculate the average fare amount and aliases the resulting column as \"average_fare\". The resulting DataFrame average_fare_per_day is sorted by year and month and is displayed using the display function. Finally, the code saves the results to a new delta table named \"average_fare_per_day\" using the write function with \"delta\" format, and \"overwrite\" mode."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, year, month, dayofmonth, avg\n","\n","# Calculate average fare amount per day\n","average_fare_per_day = (\n","    df\n","    .groupBy(year(\"lpepPickupDatetime\").alias(\"year\"), month(\"lpepPickupDatetime\").alias(\"month\"), dayofmonth(\"lpepPickupDatetime\").alias(\"day\"))\n","    .agg(avg(\"fareAmount\").alias(\"average_fare\"))\n","    .orderBy(\"year\", \"month\", \"day\")\n",")\n","display(average_fare_per_day)\n","\n","# Save the results to a new delta table\n","average_fare_per_day.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"average_fare_per_day\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T05:56:55.5725683Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T05:57:03.7327594Z\",\"execution_finish_time\":\"2023-05-04T05:57:09.0746655Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false}},{"cell_type":"markdown","source":["# 7. Custom libraries & advanced visualisation"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["Libraries provide reusable code that Apache Spark developers may want to include in their Spark application.\n","\n","Each workspace comes with a pre-installed set of libraries available in the Spark run-time and available to be used immediately in the notebook or Spark job definition. We refer to these as built-in libraries.\n","\n","Based on the user scenarios and specific needs, you can include other libraries. There are two types of libraries you may want to include:\n","\n","- Feed library: Feed libraries are the ones that come from public sources or repositories. You can install Python feed libraries from PyPI and Conda by specifying the source in the Library Management portals. You can also use a Conda environment specification .yml file to install libraries.\n","\n","- Custom library: Custom libraries are the code built by you or your organization. .whl, .jar and .tar.gz can be managed through Library Management portals. Note that .tar.gz is only supported for R language, please use .whl for Python custom libraries.\n","\n","| **Library name** | **Workspace update** | **In-line installation** |\n","|---|---|---|\n","| **Python Feed (PyPI & Conda)** | Supported | Supported |\n","| **Python Custom (.whl)** | Supported | Supported |\n","| **R Feed (CRAN)** | Not Supported | Supported |\n","| **R custom (.tar.gz)** | Supported | Supported |\n","| **Jar** | Supported | Not Supported |"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["## Install library"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["pip install altair"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T06:00:14.9752816Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T06:00:15.3100867Z\",\"execution_finish_time\":\"2023-05-04T06:00:18.0549515Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"markdown","source":["## Create custom visualisation with new library"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["import altair as alt\n","\n","df = spark.sql(\"SELECT * FROM lab140lakehouse.nyc_taxi_cleansed LIMIT 5000\")\n","\n","data = df.toPandas()\n","\n","alt.Chart(data).mark_point().encode(\n","    x='tripDistance',\n","    y='fareAmount',\n","    color='paymentType:N',\n","    tooltip=['tripDistance', 'fareAmount', 'paymentType']\n",").interactive()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T06:00:18.4413952Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T06:00:18.7661531Z\",\"execution_finish_time\":\"2023-05-04T06:00:21.1510673Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false}},{"cell_type":"code","source":["alt.Chart(data).mark_rect().encode(\n","    alt.X('tripDistance:Q', bin=True),\n","    alt.Y('fareAmount:Q', bin=True),\n","    color='count()',\n","    column='paymentType:N'\n",").interactive()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T06:00:37.0301821Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T06:00:37.3442587Z\",\"execution_finish_time\":\"2023-05-04T06:00:38.9267023Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"markdown","source":["# 8. Shortcuts and final table \n","\n","Scenario explained here..\n"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["\n","## Create shortcut to external ADLS Gen2\n","![image-alt-text](https://github.com/ekote/azure-architect/blob/master/images/5.gif?raw=true)\n","\n","\n","\n","URL: `https://buildlab140ekot.dfs.core.windows.net/`\n","\n","New Connection\n","\n","Shared Access Signature\n","\n","SAS: `?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwlacupx&se=2023-06-01T10:49:05Z&st=2023-05-04T02:49:05Z&spr=https,http&sig=0e0%2BlbFhxx3lcyz79VF272PLEzd0UdyMD348iNvBvQQ%3D`\n","\n","Name: `ExternalDataSideLoading`\n","path `/data`"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["## Load new data\n","\n","![image-alt-text](https://github.com/ekote/azure-architect/blob/master/images/6.gif?raw=true)"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["discouts_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Tables/ExternalDataSideLoading/Generated-NYC-Taxi-Green-Discounts.csv\")\n","\n","display(discouts_df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T06:23:47.0668723Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T06:23:47.3673495Z\",\"execution_finish_time\":\"2023-05-04T06:23:49.1369893Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false}},{"cell_type":"markdown","source":["## Unpivot sideloaded data\n","\n","The import pandas as pd line imports the Pandas library and assigns it an alias pd.\n","\n","Melt the discounts DataFrame: The pd.melt() function is used to convert the discouts_df PySpark DataFrame to a long format by converting date columns into rows. First, discouts_df.toPandas() is used to convert the PySpark DataFrame to a Pandas DataFrame. Then, pd.melt() takes the Pandas DataFrame, uses 'VendorID' as the identifier variable (id_vars), sets the 'date' as the variable name (var_name), and 'discount' as the value name (value_name). The melted DataFrame is stored in discouts_pd_df.\n","\n","Convert the melted DataFrame to a PySpark DataFrame: The spark.createDataFrame() function is used to convert the melted Pandas DataFrame discouts_pd_df back to a PySpark DataFrame, which is stored in the discounts_spark_df variable."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Melt discouts_df to long format\n","discouts_pd_df = pd.melt(discouts_df.toPandas(), id_vars=['VendorID'], var_name='date', value_name='discount')\n","\n","discounts_spark_df = spark.createDataFrame(discouts_pd_df)\n","\n","display(discounts_spark_df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T06:23:57.7555919Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T06:23:58.0275418Z\",\"execution_finish_time\":\"2023-05-04T06:23:58.999395Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false}},{"cell_type":"markdown","source":["## Prepare data for join"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["from pyspark.sql.functions import to_date\n","\n","nyc_taxi_df = spark.sql(\"SELECT * FROM lab140lakehouse.nyc_taxi\")\n","\n","nyc_taxi_df = nyc_taxi_df.withColumn(\"date\", to_date(\"lpepPickupDatetime\"))\n","\n","display(nyc_taxi_df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T06:24:05.7439175Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T06:24:06.140892Z\",\"execution_finish_time\":\"2023-05-04T06:24:08.8748972Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false}},{"cell_type":"markdown","source":["## Join two dataset and save result"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Create aliases for your DataFrames\n","df1_alias = nyc_taxi_df.alias(\"df1\")\n","df2_alias = discounts_spark_df.alias(\"df2\")\n","\n","# Define the join condition using the aliases\n","join_condition = [col(\"df1.vendorID\") == col(\"df2.VendorID\"), col(\"df1.date\") == col(\"df2.date\")]\n","\n","# Perform the join using the aliases\n","result_df = df1_alias.join(df2_alias, join_condition, how='inner')  # You can use other join types like 'left', 'right', 'outer', etc.\n","\n","# Select only the desired columns\n","result_df = result_df.select(\"df1.vendorID\", \"df1.lpepPickupDatetime\", \"df2.discount\")\n","\n","display(result_df)\n","\n","# Save the results to a new delta table\n","result_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"nyc_taxi_with_discounts\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Estera Kot\":{\"queued_time\":\"2023-05-04T06:26:30.3036478Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-04T06:26:30.6896856Z\",\"execution_finish_time\":\"2023-05-04T06:26:32.4172653Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false}},{"cell_type":"markdown","source":["![image-alt-text](https://github.com/ekote/azure-architect/blob/master/images/7.gif?raw=true)"],"metadata":{"nteract":{"transient":{"deleting":false}}}}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"ae7d5991-6ebf-464a-8d93-524f996fdada","known_lakehouses":[{"id":"601c2748-7103-4d7d-ae23-b7f28cd52e6e"},{"id":"57b57e37-c65f-4d79-bdf3-c941b1b043f0"},{"id":"ae7d5991-6ebf-464a-8d93-524f996fdada"}],"default_lakehouse_name":"lab140lakehouse","default_lakehouse_workspace_id":"f228abdc-2846-425e-a110-0792898a552f"}}},"nbformat":4,"nbformat_minor":0}